2024-03-10 16:06:52: Running on: cuda
2024-03-10 16:06:52: Experiment log path in: C:\Users\17301\Desktop\∫Ω∞‡—”ŒÛ‘§≤‚ µ—È\logs\udata\20240310-160652
2024-03-10 16:06:52: Experiment configs are: Namespace(seed=31, device='cuda', debug=False, running_mode='train', num_hist=12, num_pred=12, early_stop_patience=10, epochs=100, lr_init=0.001, load_model=False, best_path='None', hidden_dim=16, num_heads=8, batch_size=64, block_layers=2, input_dim=3, output_dim=2, dataset='udata', scalar_type='Standard', train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, log_dir='C:\\Users\\17301\\Desktop\\∫Ω∞‡—”ŒÛ‘§≤‚ µ—È\\logs\\udata\\20240310-160652')
2024-03-10 16:06:54: udata Dataset Load Finished.
2024-03-10 16:07:01: Batch: 0,  Train Loss: 91.46831512451172
2024-03-10 16:08:00: Batch: 50,  Train Loss: 92.7105484008789
2024-03-10 16:08:28: Batch: 100,  Train Loss: 87.53242492675781
2024-03-10 16:08:55: Batch: 150,  Train Loss: 91.32339477539062
2024-03-10 16:09:22: Batch: 200,  Train Loss: 86.7634506225586
2024-03-10 16:09:49: Batch: 250,  Train Loss: 84.49365234375
2024-03-10 16:10:16: Batch: 300,  Train Loss: 89.4798583984375
2024-03-10 16:10:44: Batch: 350,  Train Loss: 90.8951187133789
2024-03-10 16:11:11: Batch: 400,  Train Loss: 78.26318359375
2024-03-10 16:11:38: Batch: 450,  Train Loss: 80.45800018310547
2024-03-10 16:12:05: Batch: 500,  Train Loss: 81.34050750732422
2024-03-10 16:12:33: Batch: 550,  Train Loss: 81.37076568603516
2024-03-10 16:13:01: Batch: 600,  Train Loss: 87.8995361328125
2024-03-10 16:13:29: Batch: 650,  Train Loss: 79.39132690429688
2024-03-10 16:13:56: Batch: 700,  Train Loss: 84.03843688964844
2024-03-10 16:14:23: Batch: 750,  Train Loss: 78.79901885986328
2024-03-10 16:14:51: Batch: 800,  Train Loss: 83.32771301269531
2024-03-10 16:15:18: Batch: 850,  Train Loss: 81.5531005859375
2024-03-10 16:15:24: Epoch: 0, Train Loss: 84.63799539906796
2024-03-10 16:15:24: Batch: 0,  Valid Loss: 68.45811462402344
2024-03-10 16:15:33: Batch: 50,  Valid Loss: 69.50182342529297
2024-03-10 16:15:41: Batch: 100,  Valid Loss: 68.55392456054688
2024-03-10 16:15:49: Batch: 150,  Valid Loss: 91.4332275390625
2024-03-10 16:15:58: Batch: 200,  Valid Loss: 81.6410903930664
2024-03-10 16:16:06: Epoch: 0, Valid Loss: 79.77301959759794
2024-03-10 16:16:06: Batch: 0,  Train Loss: 81.37117004394531
2024-03-10 16:16:33: Batch: 50,  Train Loss: 76.78971099853516
2024-03-10 16:17:00: Batch: 100,  Train Loss: 74.72940826416016
2024-03-10 16:17:28: Batch: 150,  Train Loss: 76.119140625
2024-03-10 16:17:55: Batch: 200,  Train Loss: 78.8763198852539
2024-03-10 16:18:22: Batch: 250,  Train Loss: 75.17477416992188
2024-03-10 16:18:49: Batch: 300,  Train Loss: 79.35941314697266
2024-03-10 16:19:16: Batch: 350,  Train Loss: 78.77262115478516
2024-03-10 16:19:44: Batch: 400,  Train Loss: 70.80084228515625
2024-03-10 16:20:11: Batch: 450,  Train Loss: 73.40812683105469
2024-03-10 16:20:38: Batch: 500,  Train Loss: 72.74211883544922
2024-03-10 16:21:05: Batch: 550,  Train Loss: 72.18253326416016
2024-03-10 16:21:32: Batch: 600,  Train Loss: 76.00007629394531
2024-03-10 16:22:00: Batch: 650,  Train Loss: 75.371826171875
2024-03-10 16:22:27: Batch: 700,  Train Loss: 74.17760467529297
2024-03-10 16:22:54: Batch: 750,  Train Loss: 71.28306579589844
2024-03-10 16:23:21: Batch: 800,  Train Loss: 73.71920013427734
2024-03-10 16:23:48: Batch: 850,  Train Loss: 74.24700927734375
2024-03-10 16:23:54: Epoch: 1, Train Loss: 76.29936728953209
2024-03-10 16:23:55: Batch: 0,  Valid Loss: 64.72298431396484
2024-03-10 16:24:03: Batch: 50,  Valid Loss: 64.67534637451172
2024-03-10 16:24:11: Batch: 100,  Valid Loss: 64.56951904296875
2024-03-10 16:24:20: Batch: 150,  Valid Loss: 85.09263610839844
2024-03-10 16:24:28: Batch: 200,  Valid Loss: 74.51924133300781
2024-03-10 16:24:36: Epoch: 1, Valid Loss: 74.63014701503491
2024-03-10 16:24:37: Batch: 0,  Train Loss: 73.09207153320312
2024-03-10 16:25:04: Batch: 50,  Train Loss: 72.90740966796875
2024-03-10 16:25:31: Batch: 100,  Train Loss: 76.67996215820312
2024-03-10 16:25:58: Batch: 150,  Train Loss: 72.47557830810547
2024-03-10 16:26:25: Batch: 200,  Train Loss: 68.17456817626953
2024-03-10 16:26:53: Batch: 250,  Train Loss: 69.6015625
2024-03-10 16:27:20: Batch: 300,  Train Loss: 71.42684173583984
2024-03-10 16:27:47: Batch: 350,  Train Loss: 69.75634765625
2024-03-10 16:28:14: Batch: 400,  Train Loss: 67.30565643310547
2024-03-10 16:28:41: Batch: 450,  Train Loss: 73.27398681640625
2024-03-10 16:29:09: Batch: 500,  Train Loss: 72.84017181396484
2024-03-10 16:29:36: Batch: 550,  Train Loss: 72.26227569580078
2024-03-10 16:30:03: Batch: 600,  Train Loss: 68.4898452758789
2024-03-10 16:30:30: Batch: 650,  Train Loss: 72.2305908203125
2024-03-10 16:30:57: Batch: 700,  Train Loss: 69.76275634765625
2024-03-10 16:31:25: Batch: 750,  Train Loss: 66.5416030883789
2024-03-10 16:31:52: Batch: 800,  Train Loss: 69.79827880859375
2024-03-10 16:32:19: Batch: 850,  Train Loss: 66.68743896484375
2024-03-10 16:32:25: Epoch: 2, Train Loss: 71.03079913110578
2024-03-10 16:32:25: Batch: 0,  Valid Loss: 58.34076690673828
2024-03-10 16:32:34: Batch: 50,  Valid Loss: 59.881927490234375
2024-03-10 16:32:42: Batch: 100,  Valid Loss: 60.55851364135742
2024-03-10 16:32:51: Batch: 150,  Valid Loss: 73.72501373291016
2024-03-10 16:32:59: Batch: 200,  Valid Loss: 66.12599182128906
2024-03-10 16:33:07: Epoch: 2, Valid Loss: 67.62828130374554
2024-03-10 16:33:07: Batch: 0,  Train Loss: 65.6719970703125
2024-03-10 16:33:34: Batch: 50,  Train Loss: 65.82196044921875
2024-03-10 16:34:02: Batch: 100,  Train Loss: 68.11503601074219
2024-03-10 16:34:29: Batch: 150,  Train Loss: 67.59272766113281
2024-03-10 16:34:56: Batch: 200,  Train Loss: 67.27826690673828
2024-03-10 16:35:23: Batch: 250,  Train Loss: 68.00715637207031
2024-03-10 16:35:50: Batch: 300,  Train Loss: 69.99859619140625
2024-03-10 16:36:18: Batch: 350,  Train Loss: 67.07755279541016
2024-03-10 16:36:45: Batch: 400,  Train Loss: 65.7435073852539
2024-03-10 16:37:12: Batch: 450,  Train Loss: 66.36200714111328
2024-03-10 16:37:39: Batch: 500,  Train Loss: 66.2684097290039
2024-03-10 16:38:06: Batch: 550,  Train Loss: 69.13212585449219
2024-03-10 16:38:34: Batch: 600,  Train Loss: 67.02867889404297
2024-03-10 16:39:01: Batch: 650,  Train Loss: 71.23072052001953
2024-03-10 16:39:29: Batch: 700,  Train Loss: 65.57341766357422
2024-03-10 16:39:57: Batch: 750,  Train Loss: 67.39408111572266
2024-03-10 16:40:25: Batch: 800,  Train Loss: 65.5782241821289
2024-03-10 16:40:53: Batch: 850,  Train Loss: 65.90357208251953
2024-03-10 16:40:59: Epoch: 3, Train Loss: 67.65316748563762
2024-03-10 16:40:59: Batch: 0,  Valid Loss: 52.70397186279297
2024-03-10 16:41:09: Batch: 50,  Valid Loss: 58.08114242553711
2024-03-10 16:41:18: Batch: 100,  Valid Loss: 59.850379943847656
2024-03-10 16:41:27: Batch: 150,  Valid Loss: 72.89383697509766
2024-03-10 16:41:37: Batch: 200,  Valid Loss: 62.46661376953125
2024-03-10 16:41:45: Epoch: 3, Valid Loss: 65.86945783174954
2024-03-10 16:41:46: Batch: 0,  Train Loss: 67.49301147460938
2024-03-10 16:42:14: Batch: 50,  Train Loss: 67.3669662475586
2024-03-10 16:42:42: Batch: 100,  Train Loss: 64.56270599365234
2024-03-10 16:43:09: Batch: 150,  Train Loss: 68.87596893310547
2024-03-10 16:43:37: Batch: 200,  Train Loss: 69.37477111816406
2024-03-10 16:44:04: Batch: 250,  Train Loss: 65.38727569580078
2024-03-10 16:44:31: Batch: 300,  Train Loss: 65.58282470703125
2024-03-10 16:44:58: Batch: 350,  Train Loss: 67.4842529296875
2024-03-10 16:45:25: Batch: 400,  Train Loss: 66.71844482421875
2024-03-10 16:45:52: Batch: 450,  Train Loss: 65.33686065673828
2024-03-10 16:46:20: Batch: 500,  Train Loss: 65.90528106689453
2024-03-10 16:46:47: Batch: 550,  Train Loss: 65.7093276977539
2024-03-10 16:47:14: Batch: 600,  Train Loss: 63.87112808227539
2024-03-10 16:47:41: Batch: 650,  Train Loss: 65.55574798583984
2024-03-10 16:48:08: Batch: 700,  Train Loss: 62.025001525878906
2024-03-10 16:48:36: Batch: 750,  Train Loss: 66.0142593383789
2024-03-10 16:49:03: Batch: 800,  Train Loss: 64.41993713378906
2024-03-10 16:49:30: Batch: 850,  Train Loss: 63.15074920654297
2024-03-10 16:49:36: Epoch: 4, Train Loss: 65.44839794397907
2024-03-10 16:49:36: Batch: 0,  Valid Loss: 51.78645706176758
2024-03-10 16:49:44: Batch: 50,  Valid Loss: 56.74769973754883
2024-03-10 16:49:53: Batch: 100,  Valid Loss: 59.64435577392578
2024-03-10 16:50:01: Batch: 150,  Valid Loss: 71.29460906982422
2024-03-10 16:50:10: Batch: 200,  Valid Loss: 61.649803161621094
2024-03-10 16:50:17: Epoch: 4, Valid Loss: 64.72628839778514
2024-03-10 16:50:18: Batch: 0,  Train Loss: 64.6792984008789
2024-03-10 16:50:45: Batch: 50,  Train Loss: 64.18031311035156
2024-03-10 16:51:12: Batch: 100,  Train Loss: 65.69943237304688
2024-03-10 16:51:39: Batch: 150,  Train Loss: 64.49214935302734
2024-03-10 16:52:07: Batch: 200,  Train Loss: 65.72635650634766
2024-03-10 16:52:34: Batch: 250,  Train Loss: 61.35472869873047
2024-03-10 16:53:01: Batch: 300,  Train Loss: 64.03107452392578
2024-03-10 16:53:28: Batch: 350,  Train Loss: 64.92671203613281
2024-03-10 16:53:56: Batch: 400,  Train Loss: 62.51909637451172
2024-03-10 16:54:23: Batch: 450,  Train Loss: 63.49888229370117
2024-03-10 16:54:50: Batch: 500,  Train Loss: 61.15235137939453
2024-03-10 16:55:17: Batch: 550,  Train Loss: 61.59397506713867
2024-03-10 16:55:44: Batch: 600,  Train Loss: 60.451942443847656
2024-03-10 16:56:11: Batch: 650,  Train Loss: 63.134517669677734
2024-03-10 16:56:39: Batch: 700,  Train Loss: 62.15399169921875
2024-03-10 16:57:06: Batch: 750,  Train Loss: 65.42467498779297
2024-03-10 16:57:33: Batch: 800,  Train Loss: 61.71013641357422
2024-03-10 16:58:00: Batch: 850,  Train Loss: 62.064697265625
2024-03-10 16:58:06: Epoch: 5, Train Loss: 63.893764987069325
2024-03-10 16:58:06: Batch: 0,  Valid Loss: 51.741119384765625
2024-03-10 16:58:15: Batch: 50,  Valid Loss: 56.470523834228516
2024-03-10 16:58:23: Batch: 100,  Valid Loss: 58.76722717285156
2024-03-10 16:58:32: Batch: 150,  Valid Loss: 68.16651916503906
2024-03-10 16:58:40: Batch: 200,  Valid Loss: 59.64698028564453
2024-03-10 16:58:48: Epoch: 5, Valid Loss: 63.214262078165525
2024-03-10 16:58:48: Batch: 0,  Train Loss: 62.52534484863281
2024-03-10 16:59:15: Batch: 50,  Train Loss: 68.66399383544922
2024-03-10 16:59:43: Batch: 100,  Train Loss: 62.58698654174805
2024-03-10 17:00:10: Batch: 150,  Train Loss: 62.81661605834961
2024-03-10 17:00:37: Batch: 200,  Train Loss: 67.1414794921875
2024-03-10 17:01:04: Batch: 250,  Train Loss: 65.36262512207031
2024-03-10 17:01:31: Batch: 300,  Train Loss: 63.71854782104492
