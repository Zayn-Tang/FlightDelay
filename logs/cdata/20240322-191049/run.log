2024-03-22 19:10:49: Running on: cuda
2024-03-22 19:10:49: Experiment log path in: C:\Users\17301\Desktop\��������Ԥ��ʵ��\logs\cdata\20240322-191049
2024-03-22 19:10:49: Experiment configs are: Namespace(seed=31, device='cuda', debug=False, running_mode='train', epochs=100, lr_init=0.001, batch_size=64, load_train_model=False, train_best_path='None', load_model=False, best_path='None', early_stop_patience=5, num_hist=12, num_pred=1, d_model=32, d_output=2, dropout=0.1, shm_temp=0.6, nmb_prototype=6, aug_drop_percent=0.1, yita=0.5, grad_norm=True, max_grad_norm=7, dataset='cdata', datapath='cdata.npy', num_nodes=50, scalar_type='Standard', train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, adj_mx='dist_mx.npy', od_mx='od_mx.npy', log_dir='C:\\Users\\17301\\Desktop\\��������Ԥ��ʵ��\\logs\\cdata\\20240322-191049')
2024-03-22 19:10:50: cdata Dataset Load Finished.
2024-03-22 19:10:52: Model_Aug(
  (encoder): STEncoder(
    (tconv11): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(3, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (pooler): Pooler(
      (att): FCLayer(
        (linear): Conv2d(16, 10, kernel_size=(1, 1), stride=(1, 1))
      )
      (align): Align()
      (softmax): Softmax(dim=2)
      (agg): AvgPool2d(kernel_size=(10, 1), stride=1, padding=0)
    )
    (sconv12): SpatioConvLayer(
      (align): Align()
    )
    (tconv13): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(16, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (ln1): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (tconv21): TemporalConvLayer(
      (align): Align(
        (conv1x1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (sconv22): SpatioConvLayer(
      (align): Align()
    )
    (tconv23): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(16, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (ln2): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout2): Dropout(p=0.1, inplace=False)
    (out_conv): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(32, 64, kernel_size=(4, 1), stride=(1, 1))
    )
    (ln3): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout3): Dropout(p=0.1, inplace=False)
  )
  (mlp): MLP(
    (fc1): FCLayer(
      (linear): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
    )
    (fc2): FCLayer(
      (linear): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (thm): TemporalHeteroModel(
    (read): AvgReadout(
      (sigm): Sigmoid()
    )
    (disc): Discriminator(
      (net): Bilinear(in1_features=32, in2_features=32, out_features=1, bias=True)
    )
    (b_xent): BCEWithLogitsLoss()
  )
  (shm): SpatialHeteroModel(
    (prototypes): Linear(in_features=32, out_features=6, bias=False)
  )
)
2024-03-22 19:11:14: ****Epoch: 0, Train Loss: 204.8297097269694, MAE: 70.73878314971924, RMSE: 130.81161308288574, STH: 3.279312589764595
2024-03-22 19:11:15: ****Epoch: 0, Valid Loss: 156.9038859647863, MAE: 52.328781352323645, RMSE: 101.87375403011546, STH: 2.7013505234437827
2024-03-22 19:11:22: ****Epoch: 1, Train Loss: 175.2877340189616, MAE: 61.27999075571696, RMSE: 111.34709831237792, STH: 2.660645225942135
2024-03-22 19:11:23: ****Epoch: 1, Valid Loss: 145.12170652501723, MAE: 48.978408185173485, RMSE: 93.63790328081916, STH: 2.505394908610512
2024-03-22 19:11:30: ****Epoch: 2, Train Loss: 162.99002637227377, MAE: 57.354980583190915, RMSE: 103.08433893839518, STH: 2.5507070819536843
2024-03-22 19:11:31: ****Epoch: 2, Valid Loss: 139.37558602725758, MAE: 46.922459927727196, RMSE: 90.01116539450253, STH: 2.4419594764709474
2024-03-22 19:11:38: ****Epoch: 3, Train Loss: 156.22115669250488, MAE: 54.953838386535644, RMSE: 98.73776079813639, STH: 2.529556590517362
2024-03-22 19:11:39: ****Epoch: 3, Valid Loss: 136.76342575970818, MAE: 46.60771677353803, RMSE: 87.68271354226505, STH: 2.472995784703423
2024-03-22 19:11:47: ****Epoch: 4, Train Loss: 152.034587987264, MAE: 53.3925442759196, RMSE: 96.10024350484213, STH: 2.5418005690972008
2024-03-22 19:11:48: ****Epoch: 4, Valid Loss: 134.4761817483341, MAE: 45.4060806723202, RMSE: 86.58439124612248, STH: 2.4857106247368983
2024-03-22 19:11:55: ****Epoch: 5, Train Loss: 149.3294701639811, MAE: 52.303921674092614, RMSE: 94.47134157816569, STH: 2.554206597904364
2024-03-22 19:11:56: ****Epoch: 5, Valid Loss: 133.18063453225528, MAE: 44.829184431188246, RMSE: 85.8399273143095, STH: 2.511522980647929
2024-03-22 19:12:04: ****Epoch: 6, Train Loss: 147.26149345397948, MAE: 51.43687919616699, RMSE: 93.25875358581543, STH: 2.5658609976371127
2024-03-22 19:12:05: ****Epoch: 6, Valid Loss: 132.51974891213808, MAE: 44.19459704230813, RMSE: 85.79349849925322, STH: 2.531652853418799
2024-03-22 19:12:12: ****Epoch: 7, Train Loss: 145.62777371724448, MAE: 50.7075386428833, RMSE: 92.34584915161133, STH: 2.574386120637258
2024-03-22 19:12:13: ****Epoch: 7, Valid Loss: 131.76062056597542, MAE: 44.340812548469096, RMSE: 84.90038443172679, STH: 2.5194238263017987
2024-03-22 19:12:20: ****Epoch: 8, Train Loss: 144.28044057210286, MAE: 50.059288037618, RMSE: 91.63679448445637, STH: 2.584357629517714
2024-03-22 19:12:21: ****Epoch: 8, Valid Loss: 130.79596611471737, MAE: 43.64859872705796, RMSE: 84.62424859439625, STH: 2.5231188434011798
2024-03-22 19:12:28: ****Epoch: 9, Train Loss: 143.11102366129558, MAE: 49.493729235331216, RMSE: 91.02456769307454, STH: 2.5927261930704115
2024-03-22 19:12:29: ****Epoch: 9, Valid Loss: 130.6678166108973, MAE: 43.98592006458956, RMSE: 84.15777668672449, STH: 2.5241179399630607
2024-03-22 19:12:36: ****Epoch: 10, Train Loss: 142.3095508066813, MAE: 49.136175829569495, RMSE: 90.5730460357666, STH: 2.6003291237354276
2024-03-22 19:12:37: ****Epoch: 10, Valid Loss: 130.41690727682675, MAE: 43.48619873944451, RMSE: 84.38027101404526, STH: 2.5504381106180305
2024-03-22 19:12:45: ****Epoch: 11, Train Loss: 141.5274557240804, MAE: 48.736710421244304, RMSE: 90.17492279052735, STH: 2.615823027988275
2024-03-22 19:12:46: ****Epoch: 11, Valid Loss: 130.43881126852597, MAE: 43.52881705340217, RMSE: 84.35372121474322, STH: 2.5562732324880715
2024-03-22 19:12:54: ****Epoch: 12, Train Loss: 140.94394457499186, MAE: 48.45905011494954, RMSE: 89.86022374471028, STH: 2.6246705987056096
2024-03-22 19:12:55: ****Epoch: 12, Valid Loss: 130.186952568503, MAE: 43.35004339779125, RMSE: 84.27793148265165, STH: 2.55897699489313
2024-03-22 19:13:02: ****Epoch: 13, Train Loss: 140.28998176574706, MAE: 48.12132352828979, RMSE: 89.5340157063802, STH: 2.634642700354258
2024-03-22 19:13:03: ****Epoch: 13, Valid Loss: 129.8289473589729, MAE: 42.75608008889591, RMSE: 84.49474859798656, STH: 2.578118684011347
2024-03-22 19:13:11: ****Epoch: 14, Train Loss: 139.77916745503742, MAE: 47.891435298919674, RMSE: 89.24427459716797, STH: 2.643458654880524
2024-03-22 19:13:12: ****Epoch: 14, Valid Loss: 130.2568838680492, MAE: 43.03891177457922, RMSE: 84.63189661362591, STH: 2.586076071332483
2024-03-22 19:13:20: ****Epoch: 15, Train Loss: 139.248818359375, MAE: 47.63563288370768, RMSE: 88.96624239603679, STH: 2.64694283892711
2024-03-22 19:13:21: ****Epoch: 15, Valid Loss: 129.6894936954274, MAE: 43.00501439711627, RMSE: 84.1114527982824, STH: 2.573026283348308
2024-03-22 19:13:29: ****Epoch: 16, Train Loss: 138.84193351745606, MAE: 47.39367619832357, RMSE: 88.7922353363037, STH: 2.656022527317206
2024-03-22 19:13:30: ****Epoch: 16, Valid Loss: 130.07746851303997, MAE: 43.305774935554055, RMSE: 84.18002171235926, STH: 2.5916718816055972
2024-03-22 19:13:38: ****Epoch: 17, Train Loss: 138.39456331888834, MAE: 47.211878039042155, RMSE: 88.51585975646972, STH: 2.6668255032102266
2024-03-22 19:13:39: ****Epoch: 17, Valid Loss: 129.5938735961914, MAE: 42.57298101537368, RMSE: 84.4115307078642, STH: 2.609361412945916
2024-03-22 19:13:47: ****Epoch: 18, Train Loss: 138.11960276285808, MAE: 47.035339546203616, RMSE: 88.40735753377278, STH: 2.6769053706526758
2024-03-22 19:13:48: ****Epoch: 18, Valid Loss: 129.68965597713694, MAE: 42.50124531914206, RMSE: 84.5648520525764, STH: 2.6235581653959614
2024-03-22 19:13:56: ****Epoch: 19, Train Loss: 137.91588678995768, MAE: 46.96941395441691, RMSE: 88.26454714457194, STH: 2.6819261358181636
2024-03-22 19:13:57: ****Epoch: 19, Valid Loss: 129.78730648265164, MAE: 43.21208882051356, RMSE: 83.957922632554, STH: 2.6172947908149045
2024-03-22 19:14:05: ****Epoch: 20, Train Loss: 137.42765772501627, MAE: 46.72189809163412, RMSE: 88.01415346781413, STH: 2.691605333785216
2024-03-22 19:14:06: ****Epoch: 20, Valid Loss: 129.52635031307446, MAE: 42.640832586849434, RMSE: 84.26394747565774, STH: 2.6215695314547593
2024-03-22 19:14:13: ****Epoch: 21, Train Loss: 137.14261199951173, MAE: 46.588653831481935, RMSE: 87.8643468729655, STH: 2.689611310561498
2024-03-22 19:14:14: ****Epoch: 21, Valid Loss: 129.680247407801, MAE: 42.74758181852453, RMSE: 84.31815652286305, STH: 2.614510005011278
2024-03-22 19:14:21: ****Epoch: 22, Train Loss: 136.90162796020508, MAE: 46.46711861928304, RMSE: 87.74194061279297, STH: 2.692569211820761
2024-03-22 19:14:22: ****Epoch: 22, Valid Loss: 129.2461077521829, MAE: 42.35286465813132, RMSE: 84.25961739035213, STH: 2.633625776276869
2024-03-22 19:14:29: ****Epoch: 23, Train Loss: 136.67383211771647, MAE: 46.36086296081543, RMSE: 87.61559145609537, STH: 2.6973770593603454
2024-03-22 19:14:30: ****Epoch: 23, Valid Loss: 129.55143486471738, MAE: 42.47492016063017, RMSE: 84.4375955918256, STH: 2.6389193138655496
2024-03-22 19:14:37: ****Epoch: 24, Train Loss: 136.36854672749837, MAE: 46.197658462524416, RMSE: 87.47398531595866, STH: 2.6969027638435366
2024-03-22 19:14:38: ****Epoch: 24, Valid Loss: 130.00037877699907, MAE: 42.77762175167308, RMSE: 84.59089588838465, STH: 2.6318606832448177
2024-03-22 19:14:45: ****Epoch: 25, Train Loss: 136.10696876525878, MAE: 46.08370019276937, RMSE: 87.32202209472656, STH: 2.701247046291828
2024-03-22 19:14:46: ****Epoch: 25, Valid Loss: 129.5187312406652, MAE: 42.356032696892235, RMSE: 84.50576499490177, STH: 2.656932497024536
2024-03-22 19:14:54: ****Epoch: 26, Train Loss: 135.3544721221924, MAE: 45.75728983561198, RMSE: 86.88856386820476, STH: 2.7086180905501047
2024-03-22 19:14:55: ****Epoch: 26, Valid Loss: 129.24856657140396, MAE: 42.37470647026511, RMSE: 84.22890741684857, STH: 2.64495204652057
2024-03-22 19:15:03: ****Epoch: 27, Train Loss: 135.2448884073893, MAE: 45.70382208506266, RMSE: 86.8331251780192, STH: 2.70794152478377
2024-03-22 19:15:04: ****Epoch: 27, Valid Loss: 129.3113275864545, MAE: 42.30421003453872, RMSE: 84.36065332749311, STH: 2.646464079618454
2024-03-22 19:15:11: ****Epoch: 28, Train Loss: 135.08031127929686, MAE: 45.63301291147868, RMSE: 86.74206921895345, STH: 2.7052289826671285
2024-03-22 19:15:12: ****Epoch: 28, Valid Loss: 129.30502391142005, MAE: 42.34097761266372, RMSE: 84.32217281565947, STH: 2.641873556024888
2024-03-22 19:15:12: **************Current best model saved to C:\Users\17301\Desktop\��������Ԥ��ʵ��\logs\cdata\20240322-191049\best_model.pth
2024-03-22 19:15:12: INFLOW, MAE: 53.14, MAPE: 95.48
2024-03-22 19:15:12: OUTFLOW, MAE: 51.20, MAPE: 93.89
