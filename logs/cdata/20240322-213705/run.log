2024-03-22 21:37:05: Running on: cuda
2024-03-22 21:37:05: Experiment log path in: C:\Users\17301\Desktop\∫Ω∞‡—”ŒÛ‘§≤‚ µ—È\logs\cdata\20240322-213705
2024-03-22 21:37:05: Experiment configs are: Namespace(seed=31, device='cuda', debug=False, running_mode='train', epochs=100, lr_init=0.001, batch_size=64, load_train_model=False, train_best_path='None', load_model=False, best_path='None', early_stop_patience=5, num_hist=12, num_pred=1, d_model=32, d_output=2, dropout=0.1, shm_temp=0.6, nmb_prototype=6, aug_drop_percent=0.1, yita=0.5, grad_norm=True, max_grad_norm=7, dataset='cdata', datapath='cdata.npy', num_nodes=50, scalar_type='Standard', train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, adj_mx='dist_mx.npy', od_mx='od_mx.npy', log_dir='C:\\Users\\17301\\Desktop\\∫Ω∞‡—”ŒÛ‘§≤‚ µ—È\\logs\\cdata\\20240322-213705')
2024-03-22 21:37:06: cdata Dataset Load Finished.
2024-03-22 21:37:07: Model_Aug(
  (encoder): STEncoder(
    (tconv11): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(3, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (pooler): Pooler(
      (att): FCLayer(
        (linear): Conv2d(16, 10, kernel_size=(1, 1), stride=(1, 1))
      )
      (align): Align()
      (softmax): Softmax(dim=2)
      (agg): AvgPool2d(kernel_size=(10, 1), stride=1, padding=0)
    )
    (sconv12): SpatioConvLayer(
      (align): Align()
    )
    (tconv13): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(16, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (ln1): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (tconv21): TemporalConvLayer(
      (align): Align(
        (conv1x1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (sconv22): SpatioConvLayer(
      (align): Align()
    )
    (tconv23): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(16, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (ln2): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout2): Dropout(p=0.1, inplace=False)
    (out_conv): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(32, 64, kernel_size=(4, 1), stride=(1, 1))
    )
    (ln3): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout3): Dropout(p=0.1, inplace=False)
  )
  (mlp): MLP(
    (fc1): FCLayer(
      (linear): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
    )
    (fc2): FCLayer(
      (linear): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (thm): TemporalHeteroModel(
    (read): AvgReadout(
      (sigm): Sigmoid()
    )
    (disc): Discriminator(
      (net): Bilinear(in1_features=32, in2_features=32, out_features=1, bias=True)
    )
    (b_xent): BCEWithLogitsLoss()
  )
  (shm): SpatialHeteroModel(
    (prototypes): Linear(in_features=32, out_features=6, bias=False)
  )
)
2024-03-22 21:38:57: ****Epoch: 0, Train Loss: 204.76906010945638, MAE: 70.73157801310221, RMSE: 130.81090166727702, STH: 3.226580304702123
2024-03-22 21:38:58: ****Epoch: 0, Valid Loss: 156.78772044462318, MAE: 52.34426498413086, RMSE: 101.79998083675609, STH: 2.6434751608792473
2024-03-22 21:39:05: ****Epoch: 1, Train Loss: 175.2184294128418, MAE: 61.25394925435384, RMSE: 111.33583048502604, STH: 2.6286496308445932
2024-03-22 21:39:06: ****Epoch: 1, Valid Loss: 145.0696464987362, MAE: 49.02632740245146, RMSE: 93.56545580695658, STH: 2.477862504650565
2024-03-22 21:39:13: ****Epoch: 2, Train Loss: 162.92824045817056, MAE: 57.32650610605876, RMSE: 103.07447809855144, STH: 2.5272562705477077
2024-03-22 21:39:14: ****Epoch: 2, Valid Loss: 139.321092672909, MAE: 46.80515495749081, RMSE: 90.09402977438533, STH: 2.421907813759411
2024-03-22 21:39:22: ****Epoch: 3, Train Loss: 156.15313672383627, MAE: 54.92311799367269, RMSE: 98.71524360656738, STH: 2.5147751684983572
2024-03-22 21:39:22: ****Epoch: 3, Valid Loss: 136.71302705652573, MAE: 46.50415577607996, RMSE: 87.75968062456916, STH: 2.449190610997817
2024-03-22 21:39:30: ****Epoch: 4, Train Loss: 151.9794337717692, MAE: 53.37256568908691, RMSE: 96.07571192423502, STH: 2.5311556057135265
2024-03-22 21:39:31: ****Epoch: 4, Valid Loss: 134.42064846263213, MAE: 45.373041826135974, RMSE: 86.59005656522864, STH: 2.457551067716935
2024-03-22 21:39:38: ****Epoch: 5, Train Loss: 149.30063631693523, MAE: 52.308589464823406, RMSE: 94.44549173990886, STH: 2.5465546291073164
2024-03-22 21:39:39: ****Epoch: 5, Valid Loss: 133.11741440716912, MAE: 44.83661106334013, RMSE: 85.79954537784351, STH: 2.481259140547584
2024-03-22 21:39:46: ****Epoch: 6, Train Loss: 147.22664100646972, MAE: 51.44051617940267, RMSE: 93.21811787923177, STH: 2.5680070717136063
2024-03-22 21:39:47: ****Epoch: 6, Valid Loss: 132.51757740693932, MAE: 44.14111925012925, RMSE: 85.84827616074506, STH: 2.5281820514622857
2024-03-22 21:39:55: ****Epoch: 7, Train Loss: 145.58513170878092, MAE: 50.70829902648926, RMSE: 92.29402188618978, STH: 2.582810861468315
2024-03-22 21:39:56: ****Epoch: 7, Valid Loss: 131.72565092198988, MAE: 44.4111624100629, RMSE: 84.79809502994313, STH: 2.5163933922262753
2024-03-22 21:40:03: ****Epoch: 8, Train Loss: 144.27414896647136, MAE: 50.0759655380249, RMSE: 91.60203008015951, STH: 2.596152784327666
2024-03-22 21:40:04: ****Epoch: 8, Valid Loss: 130.92005678064683, MAE: 43.87807958266314, RMSE: 84.51530169318704, STH: 2.5266753950539758
2024-03-22 21:40:11: ****Epoch: 9, Train Loss: 143.06111333211263, MAE: 49.50397313435872, RMSE: 90.9544702911377, STH: 2.602669550180435
2024-03-22 21:40:12: ****Epoch: 9, Valid Loss: 130.74710397159353, MAE: 43.99956135469324, RMSE: 84.2104772231158, STH: 2.5370660042061526
2024-03-22 21:40:19: ****Epoch: 10, Train Loss: 142.29363596598307, MAE: 49.14822241465251, RMSE: 90.5360797627767, STH: 2.6093337257703144
2024-03-22 21:40:19: ****Epoch: 10, Valid Loss: 130.4150790943819, MAE: 43.513309119729435, RMSE: 84.34485895493451, STH: 2.556911537226509
2024-03-22 21:40:26: ****Epoch: 11, Train Loss: 141.4620967102051, MAE: 48.733015721639, RMSE: 90.10782376607258, STH: 2.621256840030352
2024-03-22 21:40:27: ****Epoch: 11, Valid Loss: 130.64674898035386, MAE: 43.6439764808206, RMSE: 84.44092550838695, STH: 2.5618470879162056
2024-03-22 21:40:34: ****Epoch: 12, Train Loss: 140.91059117635092, MAE: 48.46629035313924, RMSE: 89.81115028381348, STH: 2.633150830467542
2024-03-22 21:40:35: ****Epoch: 12, Valid Loss: 130.31898911420038, MAE: 43.60998701207778, RMSE: 84.13802763995002, STH: 2.570974911661709
2024-03-22 21:40:42: ****Epoch: 13, Train Loss: 140.28952328999839, MAE: 48.141786651611326, RMSE: 89.50438552856446, STH: 2.643351479669412
2024-03-22 21:40:43: ****Epoch: 13, Valid Loss: 129.73472469554227, MAE: 42.916932364071116, RMSE: 84.23046183866613, STH: 2.587329019869075
2024-03-22 21:40:50: ****Epoch: 14, Train Loss: 139.7823296101888, MAE: 47.91008244832357, RMSE: 89.2158364613851, STH: 2.656410050392151
2024-03-22 21:40:51: ****Epoch: 14, Valid Loss: 130.0824317483341, MAE: 42.908668091717885, RMSE: 84.58117859784295, STH: 2.592584600168116
2024-03-22 21:40:58: ****Epoch: 15, Train Loss: 139.19779042561848, MAE: 47.64055850346883, RMSE: 88.89792002360026, STH: 2.659311012327671
2024-03-22 21:40:59: ****Epoch: 15, Valid Loss: 129.582798946605, MAE: 42.82956628238453, RMSE: 84.15980130363913, STH: 2.593431836016038
2024-03-22 21:41:06: ****Epoch: 16, Train Loss: 138.84260088602701, MAE: 47.41475195566813, RMSE: 88.75932243347168, STH: 2.6685258747140566
2024-03-22 21:41:07: ****Epoch: 16, Valid Loss: 129.757532007554, MAE: 43.02455092037425, RMSE: 84.12723604090074, STH: 2.6057450347086966
2024-03-22 21:41:14: ****Epoch: 17, Train Loss: 138.3714580790202, MAE: 47.22109536488851, RMSE: 88.4700949605306, STH: 2.680266980826855
2024-03-22 21:41:15: ****Epoch: 17, Valid Loss: 129.33134630988627, MAE: 42.625916335161996, RMSE: 84.08892526065603, STH: 2.6165059128228356
2024-03-22 21:41:22: ****Epoch: 18, Train Loss: 138.17061030069988, MAE: 47.09008949279785, RMSE: 88.39191436767578, STH: 2.6886060191194217
2024-03-22 21:41:23: ****Epoch: 18, Valid Loss: 129.42242359834557, MAE: 42.37105138442096, RMSE: 84.427961955351, STH: 2.6234102974919713
2024-03-22 21:41:30: ****Epoch: 19, Train Loss: 137.8370166269938, MAE: 46.95444938023885, RMSE: 88.19291770935058, STH: 2.689649648666382
2024-03-22 21:41:31: ****Epoch: 19, Valid Loss: 129.317322585162, MAE: 42.83912099950454, RMSE: 83.8522294885972, STH: 2.6259718071011937
2024-03-22 21:41:38: ****Epoch: 20, Train Loss: 137.41395144144693, MAE: 46.72830500920614, RMSE: 87.98427508036296, STH: 2.7013715929786364
2024-03-22 21:41:39: ****Epoch: 20, Valid Loss: 129.3318604413201, MAE: 42.55483887616326, RMSE: 84.14714184929343, STH: 2.629880379929262
2024-03-22 21:41:47: ****Epoch: 21, Train Loss: 137.2004862467448, MAE: 46.62814313252767, RMSE: 87.86888356526693, STH: 2.703458804686864
2024-03-22 21:41:48: ****Epoch: 21, Valid Loss: 129.28408651912915, MAE: 42.504229130464445, RMSE: 84.14320921056411, STH: 2.636647176041323
2024-03-22 21:41:55: ****Epoch: 22, Train Loss: 136.98277216593425, MAE: 46.50817244847615, RMSE: 87.76342712402344, STH: 2.711172249118487
2024-03-22 21:41:56: ****Epoch: 22, Valid Loss: 128.83398993997014, MAE: 42.261695076437555, RMSE: 83.92307896333583, STH: 2.6492155032999376
2024-03-22 21:42:04: ****Epoch: 23, Train Loss: 136.73067982991537, MAE: 46.382256711324054, RMSE: 87.63267044067383, STH: 2.715752931435903
2024-03-22 21:42:05: ****Epoch: 23, Valid Loss: 129.16412829230813, MAE: 42.48588232152602, RMSE: 84.02620598288144, STH: 2.652039499843822
2024-03-22 21:42:12: ****Epoch: 24, Train Loss: 136.42435351053874, MAE: 46.21767958323161, RMSE: 87.49345443725586, STH: 2.7132186673084893
2024-03-22 21:42:12: ****Epoch: 24, Valid Loss: 129.48966127283433, MAE: 42.52622658224667, RMSE: 84.31861935783834, STH: 2.644815408482271
2024-03-22 21:42:19: ****Epoch: 25, Train Loss: 136.17131591796874, MAE: 46.117457974751794, RMSE: 87.33616724650065, STH: 2.717690116961797
2024-03-22 21:42:20: ****Epoch: 25, Valid Loss: 129.4252731323242, MAE: 42.04841494840734, RMSE: 84.70489439122817, STH: 2.6719632162767297
2024-03-22 21:42:27: ****Epoch: 26, Train Loss: 135.3961698659261, MAE: 45.76463272094727, RMSE: 86.9098513285319, STH: 2.7216863145430885
2024-03-22 21:42:28: ****Epoch: 26, Valid Loss: 128.904600165872, MAE: 42.250818678912, RMSE: 83.9963519376867, STH: 2.6574292070725383
2024-03-22 21:42:34: ****Epoch: 27, Train Loss: 135.2683312479655, MAE: 45.71489115397135, RMSE: 86.83112889607747, STH: 2.722311057249705
2024-03-22 21:42:35: ****Epoch: 27, Valid Loss: 128.94564065372242, MAE: 42.139829747817096, RMSE: 84.14429810467888, STH: 2.661513602032381
2024-03-22 21:42:43: ****Epoch: 28, Train Loss: 135.11823148091634, MAE: 45.645575281778974, RMSE: 86.75124727884928, STH: 2.721409318546454
2024-03-22 21:42:44: ****Epoch: 28, Valid Loss: 128.93376446892233, MAE: 42.22654026255888, RMSE: 84.05247380873736, STH: 2.6547517643255345
2024-03-22 21:42:44: **************Current best model saved to C:\Users\17301\Desktop\∫Ω∞‡—”ŒÛ‘§≤‚ µ—È\logs\cdata\20240322-213705\best_model.pth
2024-03-22 21:42:44: INFLOW, MAE: 50.09, RMSE: 91.8319
2024-03-22 21:42:44: OUTFLOW, MAE: 50.29, RMSE: 93.3086
2024-03-22 21:42:44: Test Error: MAE 50.190290451049805,  RMSE 92.57022094726562
