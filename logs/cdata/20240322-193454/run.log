2024-03-22 19:34:54: Running on: cuda
2024-03-22 19:34:54: Experiment log path in: C:\Users\17301\Desktop\��������Ԥ��ʵ��\logs\cdata\20240322-193454
2024-03-22 19:34:54: Experiment configs are: Namespace(seed=31, device='cuda', debug=False, running_mode='train', epochs=100, lr_init=0.001, batch_size=64, load_train_model=False, train_best_path='None', load_model=False, best_path='None', early_stop_patience=5, num_hist=12, num_pred=1, d_model=32, d_output=2, dropout=0.1, shm_temp=0.6, nmb_prototype=6, aug_drop_percent=0.1, yita=0.5, grad_norm=True, max_grad_norm=7, dataset='cdata', datapath='cdata.npy', num_nodes=50, scalar_type='Standard', train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, adj_mx='dist_mx.npy', od_mx='od_mx.npy', log_dir='C:\\Users\\17301\\Desktop\\��������Ԥ��ʵ��\\logs\\cdata\\20240322-193454')
2024-03-22 19:34:55: cdata Dataset Load Finished.
2024-03-22 19:34:56: Model_Aug(
  (encoder): STEncoder(
    (tconv11): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(3, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (pooler): Pooler(
      (att): FCLayer(
        (linear): Conv2d(16, 10, kernel_size=(1, 1), stride=(1, 1))
      )
      (align): Align()
      (softmax): Softmax(dim=2)
      (agg): AvgPool2d(kernel_size=(10, 1), stride=1, padding=0)
    )
    (sconv12): SpatioConvLayer(
      (align): Align()
    )
    (tconv13): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(16, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (ln1): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (tconv21): TemporalConvLayer(
      (align): Align(
        (conv1x1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (sconv22): SpatioConvLayer(
      (align): Align()
    )
    (tconv23): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(16, 32, kernel_size=(3, 1), stride=(1, 1))
    )
    (ln2): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout2): Dropout(p=0.1, inplace=False)
    (out_conv): TemporalConvLayer(
      (align): Align()
      (conv): Conv2d(32, 64, kernel_size=(4, 1), stride=(1, 1))
    )
    (ln3): LayerNorm((50, 32), eps=1e-05, elementwise_affine=True)
    (dropout3): Dropout(p=0.1, inplace=False)
  )
  (mlp): MLP(
    (fc1): FCLayer(
      (linear): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
    )
    (fc2): FCLayer(
      (linear): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (thm): TemporalHeteroModel(
    (read): AvgReadout(
      (sigm): Sigmoid()
    )
    (disc): Discriminator(
      (net): Bilinear(in1_features=32, in2_features=32, out_features=1, bias=True)
    )
    (b_xent): BCEWithLogitsLoss()
  )
  (shm): SpatialHeteroModel(
    (prototypes): Linear(in_features=32, out_features=6, bias=False)
  )
)
2024-03-22 19:35:05: ****Epoch: 0, Train Loss: 204.79364451090495, MAE: 70.7355001449585, RMSE: 130.8123366800944, STH: 3.2458076544602714
2024-03-22 19:35:06: ****Epoch: 0, Valid Loss: 156.83622858384075, MAE: 52.30872472875259, RMSE: 101.86860549029181, STH: 2.658897604661829
2024-03-22 19:35:13: ****Epoch: 1, Train Loss: 175.2491222635905, MAE: 61.26739678700765, RMSE: 111.34291984558105, STH: 2.6388059338927268
2024-03-22 19:35:14: ****Epoch: 1, Valid Loss: 145.09409879796644, MAE: 48.97070768019732, RMSE: 93.63851125380572, STH: 2.484879710744409
2024-03-22 19:35:21: ****Epoch: 2, Train Loss: 162.9619279988607, MAE: 57.34797997792562, RMSE: 103.08696701049804, STH: 2.526980499525865
2024-03-22 19:35:22: ****Epoch: 2, Valid Loss: 139.38396346148323, MAE: 46.84345745759852, RMSE: 90.11333815630745, STH: 2.427168072672451
2024-03-22 19:35:29: ****Epoch: 3, Train Loss: 156.18779304504395, MAE: 54.942813084920246, RMSE: 98.73166943868002, STH: 2.5133105933666227
2024-03-22 19:35:30: ****Epoch: 3, Valid Loss: 136.70498863669002, MAE: 46.45870226691751, RMSE: 87.78818283081054, STH: 2.458103174672407
2024-03-22 19:35:36: ****Epoch: 4, Train Loss: 152.0187294514974, MAE: 53.39075099945068, RMSE: 96.09738357543945, STH: 2.53059507638216
2024-03-22 19:35:37: ****Epoch: 4, Valid Loss: 134.4379721248851, MAE: 45.435850367826575, RMSE: 86.52633837531594, STH: 2.475783334760105
2024-03-22 19:35:45: ****Epoch: 5, Train Loss: 149.33727963765463, MAE: 52.31143459320069, RMSE: 94.4787927500407, STH: 2.547052314281464
2024-03-22 19:35:46: ****Epoch: 5, Valid Loss: 133.09541051528032, MAE: 44.83963847440832, RMSE: 85.76462097167969, STH: 2.4911530435085294
2024-03-22 19:35:53: ****Epoch: 6, Train Loss: 147.24236188252766, MAE: 51.44562754313151, RMSE: 93.24257944742838, STH: 2.554155313372612
2024-03-22 19:35:54: ****Epoch: 6, Valid Loss: 132.4011319328757, MAE: 44.12111704209271, RMSE: 85.76250906551586, STH: 2.51750594587887
2024-03-22 19:36:01: ****Epoch: 7, Train Loss: 145.5936075846354, MAE: 50.70188158671061, RMSE: 92.32845708211264, STH: 2.563268977999687
2024-03-22 19:36:02: ****Epoch: 7, Valid Loss: 131.63005182602825, MAE: 44.27731756322524, RMSE: 84.8440320632037, STH: 2.50870262910338
2024-03-22 19:36:09: ****Epoch: 8, Train Loss: 144.24264233907064, MAE: 50.04995180765788, RMSE: 91.61748062133789, STH: 2.5752100884914397
2024-03-22 19:36:10: ****Epoch: 8, Valid Loss: 130.82527241426357, MAE: 43.7885435665355, RMSE: 84.52151691212373, STH: 2.5152124983422897
2024-03-22 19:36:17: ****Epoch: 9, Train Loss: 143.07303619384766, MAE: 49.496105982462566, RMSE: 90.9935158030192, STH: 2.5834146099289255
2024-03-22 19:36:18: ****Epoch: 9, Valid Loss: 130.6516492058249, MAE: 43.90529255586512, RMSE: 84.2299300249885, STH: 2.516426568171557
2024-03-22 19:36:25: ****Epoch: 10, Train Loss: 142.27237353006998, MAE: 49.13778958638509, RMSE: 90.5457460784912, STH: 2.588838497896989
2024-03-22 19:36:26: ****Epoch: 10, Valid Loss: 130.15052472282858, MAE: 43.51754702399759, RMSE: 84.09879724839155, STH: 2.5341791275669547
2024-03-22 19:36:33: ****Epoch: 11, Train Loss: 141.43593691507976, MAE: 48.71617234547933, RMSE: 90.11855898539226, STH: 2.601205410460631
2024-03-22 19:36:34: ****Epoch: 11, Valid Loss: 130.22688930735868, MAE: 43.610537652408375, RMSE: 84.08315128999598, STH: 2.5332009964129503
2024-03-22 19:36:41: ****Epoch: 12, Train Loss: 140.89419075012208, MAE: 48.45377745310466, RMSE: 89.82894269307454, STH: 2.6114706900715827
2024-03-22 19:36:42: ****Epoch: 12, Valid Loss: 130.18406165627871, MAE: 43.53820255503935, RMSE: 84.10358909158145, STH: 2.5422685255022612
2024-03-22 19:36:49: ****Epoch: 13, Train Loss: 140.2642175801595, MAE: 48.13384119669596, RMSE: 89.50854728698731, STH: 2.6218279873331385
2024-03-22 19:36:50: ****Epoch: 13, Valid Loss: 129.86615932688994, MAE: 42.61975111119887, RMSE: 84.6793887867647, STH: 2.5670186803621404
2024-03-22 19:36:57: ****Epoch: 14, Train Loss: 139.7552326965332, MAE: 47.89656220753988, RMSE: 89.2234641011556, STH: 2.635206110278765
2024-03-22 19:36:58: ****Epoch: 14, Valid Loss: 130.09745303883273, MAE: 42.89757681453929, RMSE: 84.62416713938994, STH: 2.5757095508715686
2024-03-22 19:37:04: ****Epoch: 15, Train Loss: 139.15201985677083, MAE: 47.607057393391926, RMSE: 88.90284614562988, STH: 2.642115465601285
2024-03-22 19:37:05: ****Epoch: 15, Valid Loss: 129.70482832964728, MAE: 42.91027412414551, RMSE: 84.2208776586196, STH: 2.573675983793595
2024-03-22 19:37:12: ****Epoch: 16, Train Loss: 138.77835393269856, MAE: 47.38415139516194, RMSE: 88.74156110127767, STH: 2.6526414022843046
2024-03-22 19:37:13: ****Epoch: 16, Valid Loss: 129.99261277142693, MAE: 43.26378454320571, RMSE: 84.14088435453527, STH: 2.587944312656627
2024-03-22 19:37:19: ****Epoch: 17, Train Loss: 138.36221150716145, MAE: 47.21346972147624, RMSE: 88.48503089904786, STH: 2.6637107972304026
2024-03-22 19:37:20: ****Epoch: 17, Valid Loss: 129.41866024241727, MAE: 42.58465823005228, RMSE: 84.23203941794003, STH: 2.6019613907617685
2024-03-22 19:37:27: ****Epoch: 18, Train Loss: 138.10823232014974, MAE: 47.05505349477132, RMSE: 88.37965057373047, STH: 2.6735286400715514
2024-03-22 19:37:28: ****Epoch: 18, Valid Loss: 129.4666238223805, MAE: 42.68660325443044, RMSE: 84.16848274679745, STH: 2.6115389880012065
2024-03-22 19:37:35: ****Epoch: 19, Train Loss: 137.85694676717122, MAE: 46.94739811579387, RMSE: 88.23127573649089, STH: 2.6782727620005606
2024-03-22 19:37:36: ****Epoch: 19, Valid Loss: 129.45176544189454, MAE: 42.88995123470531, RMSE: 83.94567018396714, STH: 2.6161455543602217
2024-03-22 19:37:44: ****Epoch: 20, Train Loss: 137.38402506510417, MAE: 46.70746456782023, RMSE: 87.98731328328451, STH: 2.689246621032556
2024-03-22 19:37:45: ****Epoch: 20, Valid Loss: 129.69483696432675, MAE: 42.85139433916877, RMSE: 84.22992208144244, STH: 2.6135199112050675
2024-03-22 19:37:53: ****Epoch: 21, Train Loss: 136.55336229960125, MAE: 46.34259157816569, RMSE: 87.52172136942545, STH: 2.6890485243002575
2024-03-22 19:37:53: ****Epoch: 21, Valid Loss: 129.17408860150505, MAE: 42.41177994223202, RMSE: 84.14126380471622, STH: 2.6210440386744107
2024-03-22 19:38:01: ****Epoch: 22, Train Loss: 136.36912493387857, MAE: 46.25215741475423, RMSE: 87.42445236206055, STH: 2.692514955997467
2024-03-22 19:38:02: ****Epoch: 22, Valid Loss: 129.11235270780676, MAE: 42.30410409815171, RMSE: 84.17859375897577, STH: 2.6296552766771875
2024-03-22 19:38:09: ****Epoch: 23, Train Loss: 136.25768898010253, MAE: 46.19676996866862, RMSE: 87.367446975708, STH: 2.6934720212221146
2024-03-22 19:38:10: ****Epoch: 23, Valid Loss: 129.10716094970704, MAE: 42.40008475359748, RMSE: 84.07535795323989, STH: 2.6317180423175586
2024-03-22 19:38:17: ****Epoch: 24, Train Loss: 136.20707473754882, MAE: 46.15109917958578, RMSE: 87.36429272969563, STH: 2.6916818242271745
2024-03-22 19:38:18: ****Epoch: 24, Valid Loss: 129.16203909481274, MAE: 42.37279310787425, RMSE: 84.16248361924116, STH: 2.6267631590366363
2024-03-22 19:38:24: ****Epoch: 25, Train Loss: 136.13275026957194, MAE: 46.131924057006835, RMSE: 87.3064389292399, STH: 2.6943865511814753
2024-03-22 19:38:25: ****Epoch: 25, Valid Loss: 129.18673732981964, MAE: 42.40301545087029, RMSE: 84.14975962919347, STH: 2.633961518371806
2024-03-22 19:38:32: ****Epoch: 26, Train Loss: 136.06393633524576, MAE: 46.08819451649984, RMSE: 87.27707946777343, STH: 2.6986618823806445
2024-03-22 19:38:33: ****Epoch: 26, Valid Loss: 129.14392745074105, MAE: 42.37104918536018, RMSE: 84.140467116412, STH: 2.632411251348608
2024-03-22 19:38:40: ****Epoch: 27, Train Loss: 136.11785265604655, MAE: 46.11347011566162, RMSE: 87.30634450276693, STH: 2.698038253088792
2024-03-22 19:38:41: ****Epoch: 27, Valid Loss: 129.12928457821116, MAE: 42.366237976971796, RMSE: 84.13012053545783, STH: 2.6329260493026063
2024-03-22 19:38:48: ****Epoch: 28, Train Loss: 136.01515408833822, MAE: 46.07181608835856, RMSE: 87.2473698425293, STH: 2.695967764258385
2024-03-22 19:38:49: ****Epoch: 28, Valid Loss: 129.12166371065027, MAE: 42.36363184311811, RMSE: 84.12949882956111, STH: 2.6285318111672122
2024-03-22 19:38:56: ****Epoch: 29, Train Loss: 136.07553843180338, MAE: 46.09906155904134, RMSE: 87.279488474528, STH: 2.696988616188367
2024-03-22 19:38:57: ****Epoch: 29, Valid Loss: 129.11825166590074, MAE: 42.36795012530158, RMSE: 84.12143797032974, STH: 2.628864107061835
2024-03-22 19:38:57: **************Current best model saved to C:\Users\17301\Desktop\��������Ԥ��ʵ��\logs\cdata\20240322-193454\best_model.pth
2024-03-22 19:42:52: INFLOW, MAE: 50.11, RMSE: 91.4682
2024-03-22 19:42:52: OUTFLOW, MAE: 50.55, RMSE: 93.5141
2024-03-22 19:42:52: Test Error: MAE 50.33266830444336,  RMSE 92.49113845825195
